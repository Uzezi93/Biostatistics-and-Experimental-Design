---
title: "Inference and Likelihood"
author: "Uzezi Okinedo"
date: "10/20/2020"
output: html_document
---

### <span style="color: red;"> Remarkable work!! 34/18 points </span>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries}

# load all required libraries
library(tidyr)
library(bbmle)
library(dplyr)
library(broom)
library(ggplot2)
library(rayshader)
library(MASS)
library(profileModel)

```

**0.** Go through the faded examples in the lab. You don’t need to put the output here - just, do them! And let us know if you have any remaining questions, or you feel comfortable. Full credit!

i) What does the qplot of the fitted and residual values explain? Won't it make more sense to separate the residual from the fitted values with colors or something to easily assess the disparities that might exist?

ii) For making grids, what criteria are used in selecting numbers for crossing?

iii) What is the "length.out" argument used for? How is it different from the "by" argument?

iv) How is the response variable determined? I'm still not sure if my approach for determining this is correct.


**1.** Would you say you naturally gravitate towards deductive or inductive inference? Why?

I naturally gravitate towards inductive inference because I am inclined to observing patterns of occurrence before assuming any proposed explanation or hypothesis.Furthermore, my explanations must be validated through experiments before coming to a conclusion or before postulating a theorem. 

### <span style="color: red;"> 3/3 points </span>

**2.** We talked about strictly interpreted Popperian Falsification versus Lakatos’s view of a research program this week.

**2a.** Do you more strongly identify with one of these paradigms? Why? +1 EC for direct quotes (if you want to do some additional reading)

"...in science, a ‘theory’ is really a succession of slightly different theories and experimental techniques developed over time that all share a common hard core..." -Imre Lakatos. 

I strongly agree and identify with this paradigm of **Imre Lakatos** in the sense that I view scientific knowledge as being progressive and centered around a base knowledge or "common hardcore". Theories should not be totally discarded but worked on through a research program leading to the accumulation of novel knowledge and improved experimental techniques.

### <span style="color: red;"> 4/3 points </span>

**2b.** How does your own research program fit into one of these paradigms?

My research program best aligns with Imre Lakatos' paradigm of a progressive research program.

For my undergrad and Masters program, I worked on genetic diversity assessment of some plant species using less novel techniques that involved the use of Random Amplified Polymorphic DNA (RAPD) and Simple Sequence Repeats (SSR). Although, the results obtained from this research suggested significant levels of genetic diversity in these species, it was limited in revealing the depth of diversity and the relationship with adaption and selection.

Here at UMass Boston, my research still centers on "genetic diversity" which is my "common hardcore" with respect to adaptation and selection in plant species. The only difference is that my experimental approaches have become more sophisticated to include techniques like GBS, RADSeq and RNA-seq with the addition of better computational analysis tools like R and Python.

### <span style="color: red;"> 3/3 points </span>

**EC x4 2c.** This has been a shallow dive into Lakatos and Popper. Look them or other philosophers of science and inference up - Kuhn, Feyerabend, Musgrave, Deb Mayo, Sabra, Fillies, and others. What’s their core idea, and why do you agree or disagree?

i) **Popper**: Popper idealized science as progressing through a process of **falsification** and that theories whose predictions conflicted with experimental observation are discarded immediately. Therefore, he describes scientific progress as a process of **elimination**. 

I strongly disagree with this paradigm because science theories should be developed through further experiments or research and not eliminated.

ii) **Kuhn**: Kuhn postulated that science consisted of periods. He described a period called ‘**normal science**’, in which experiment and theory are performed within a particular paradigm, where scientists hold on to their theories despite anomalies. Very often, the trending paradigm is overturned, but even when such a shift happens, it does not rely on logic alone because observation is influenced by the paradigm in which it happens.

I strongly disagree with this paradigm because science should not be influenced by societal or cultural trends and results should not be manipulated to fit any generally accepted ideology or norm.

iii) **Feyerabend**: He developed the _anarchistic philosophy of science_. Feyerabend concludes that the progress of science cannot be ascertained in terms of one set of methodological rules that is always used by scientists; such a ”scientific method’ would in fact limit the activities of scientists and hamper scientific progress.

I strongly agree with the ideology of Feyerabend because he views scientific progress from the perspective of both Lakatos and Kuhn. As much the progress of science relies heavily on a research program, it is not totally excluded from societal influence. 

### <span style="color: red;"> Nice! 4/4 points </span>

##Puffers!##

**3.** Grid Sampling! Based on Friday’s lab, load up the pufferfish data and use grid sampling to find the MLE of the slope, intercept and residual SD of this model. Feel free to eyeball results from an lm() fit to get reasonable values. Try not to do this for a grid of more than ~100K points (more if you want!). It’s ok to be coarse. Compare to lm.

```{r pufferfish data}

# download pufferfish data
download.file(url = "http://biol607.github.io/homework/data/16q11PufferfishMimicry%20Caley%20&%20Schluter%202003.csv",
              destfile = "raw_data/pufferfish.csv")

```

```{r read pufferfish data}

# read in pufferfish data and assign object
pufferfish <- read.csv("./raw_data/pufferfish.csv")

# view data
pufferfish

pufferfish_lm = lm(predators ~ resemblance, data = pufferfish)

# Get coefficient estimates
tidy(pufferfish_lm)

# Get estimate for RMSE
glance(pufferfish_lm)

```

```{r MLE of slope, intercept and SD}

# slope(b0) = 3.0
# intercept(b1) = 2.0
# SD(sigma) = 3.1

# A function, given a set of values, returns the log likelihood
norm_loglik = function(b0, b1, sigma) { 

  # Compute yhats and residuals
  fit = b0 + b1 * pufferfish$resemblance
  error = pufferfish$predators - fit
  
  # Compute the log-likelihood
  log_lik = sum(dnorm(error, fit, sd = sigma, log = TRUE))
  
  
  return(log_lik)
  
} 

# Use crossing to generate testing parameters
pufferfish_norm <- crossing(b0 = seq(0.1, 2.0, by= 0.1),
                            b1 = seq(0.5, 2.5, by = 0.1),
                            sigma = seq(1.1, 3.1, by = 0.1)) %>%
  rowwise() %>%
  mutate(log_lik = norm_loglik(b0, b1, sigma)) %>%
  ungroup()

# Get MLE
pufferfish_norm %>%
  filter(log_lik == max(log_lik))

```
The log likelihood obtained from the lm analysis is -49.64899	which is pretty close to the  log likelihood value (-49.6492) obtained using grid crossing.

### <span style="color: red;"> Nice use of glance! 4/3 points </span>

**4.** Surfaces! Filter the dataset to the MLE of the SD. Plot the surface for the slope and intercept in whatever way you find most compelling. You might want to play around with zooming in to different regions, etc. Have fun!

```{r plot surface for MLE of SD}

# visualize with contour plot!
ggplot(data = pufferfish_norm %>% filter(log_lik > max(log_lik) - 3),
       mapping = aes(x = b0, y = b1, z = log_lik)) +
  geom_contour_filled(bins = 20) 

```

### <span style="color: red;"> Nice plot! 4/3 points </span>

**5.** GLM! Now, compare those results to results from glm. Show the profiles and confidence intervals from glm() for the slope and intercept. Also show how you validate assumptions.

```{r compare results to GLM}

#using glm to show profiles
pufferfish_mle <- glm(predators ~ resemblance,
                data = pufferfish,
                family = gaussian(link = "identity"))

#view pufferfish_mle
pufferfish_mle

# show profiles with CI from glm
prof <- profileModel(pufferfish_mle,
                     objective = "ordinaryDeviance",
                     quantile = qchisq(0.95, 1))

#plot profiles with grid points
plot(prof, print.grid.points = TRUE)

```

```{r validate assumptions}

#validate assumptions
#Tau test using the signed square root of the deviance
prof_mass <- profile(pufferfish_mle)
plot(prof_mass)

```
The profile function is used to test the validity of a profile. This test produces a straight line from the parabola making visualization of deviance much more possible, hence it is a good profile.

### <span style="color: red;"> 3/3 points </span>

**EC 6.** Get Outside of GLM! So, often, we have more complex models than the above. There are a variety of optimizers out there, and packages for accessing them. One of the best is bbmle by Ecologist Ben Bolker (whose dad is emeritus at UMB in computer science! Go visit him! He’s fantastic!)

Load up 'bbmle and try out mle2. It’s a bit different, in that the first argument is a function that minimizes the log likelihood (not maximizes). The second argument is a list of start values - e.g. list(slope = 2, intercept = 5, resid_sd = 2). Try and fit your model with mle2 using start values close to the actual estimates. Look at the summary and plot the profile. Note, you might get a lot of errors because it will try impossible values of your residual SD. Also, note that you’ll have to rewrite your likelihood function to return the negative log likelihood (or write a wrapper that does so). A small thing

```{r fit model with mle2}

# rewrite likelihood function to return the negative log likelihood
neg_log_lik = function(slope, intercept, resid_sd) { 

  # Compute fit
  fit = slope + intercept * pufferfish$resemblance
  error = pufferfish$predators - fit
  
  # Compute the negative log-likelihood
  neg_log_L = -sum(error, fit, sd = resid_sd, log = TRUE)
  
  return(neg_log_L)
  
} 

# test function
neg_log_lik(2, 5, 2)

```


**EC 6a.** Start values! What happens if you start with start values very far away from the initial values. Failing here is fine. But what do you think is happening, and what does this say about the value of start values?

```{r try different start values}

# Fit model using MLE
mle_results = mle2(minuslogl = neg_log_lik, start = list(slope = 50, intercept = 100, resid_sd = 50))

# View results
summary(mle_results)

# check profile
profile(mle_results)

```
It seems these start values are not suitable for fitting this model as there was no convergence in the fit.

### <span style="color: red;"> 3/3 points </span>

**EC 6b** Algorithms! By default, mle2 uses the Nelder-Mead algorithm via the optim function. What happens if you add a method argument to “SANN” or “L-BFGS-B” (and for the later, which is bounded sampling, give it a lower argument for your residual value, so it’s always positive). See ?optim for some more guidance. Do these both converge to the same value? Based on their profiles, do you trust them? (Note, Simulated annealing takes a looooong time. Go have a cuppa while the profile for that one runs).

```{r add method argument}

params <- list()    # set up empty list to store parameters
params$slope=1          # fill the list with the "best fit" parameter set from above (this is still just an educated guess)   
params$intercept=2 
params$resid_sd=3.5

# view parameters
params

# create a function for deterministic exponential decline (assuming slope is negative)
Deterministic_component <- function(xvals, slope, intercept){
  yexp <- slope*exp(intercept*xvals)        
  return(yexp)
}


# rewrite likelihood function to return the negative log likelihood using the deterministic_component function created above
neg_log_lik_02 <- function(params,df,yvar,xvar){
  
  neg_log <- -sum(dnorm(df[,yvar],Deterministic_component(df[,xvar],params['slope'],params['intercept']),sqrt(params['resid_sd']),log=TRUE))
  return(neg_log)
}
neg_log_lik_02(unlist(params),df=pufferfish,yvar="predators",xvar="resemblance")

# use optim to optimize function and set method = "SANN"
MLE_01 <- optim(fn=neg_log_lik_02,par=unlist(params),df=pufferfish,yvar="predators",xvar="resemblance",control=list(fnscale=-1), method = "SANN") 

# view optimized parameters
MLE_01$par

```

```{r “L-BFGS-B” argument}

# use optim to optimize function and set method = "L-BFGS-B"
MLE_02 <- optim(fn=neg_log_lik_02,par=unlist(params),df=pufferfish,yvar="predators",xvar="resemblance",control=list(fnscale=-1),lower=c(0.1,0.5), upper=c(2.5,3.5), hessian = TRUE, method = "L-BFGS-B") 

# view parameters from this optimization
MLE_02$par


```
These two methods "SANN" and “L-BFGS-B” do not converge at the same values.

These methods cannot be trusted because the "SANN" method gives values that totally vary from any expected value for the slope and intercept. On the other hand, the “L-BFGS-B” gives values that are only within the box-constraint of numbers provided as there's a probability of having values outside the specified numbers.

### <span style="color: red;"> 3/3 points </span>

[GitHub Extra Credit](https://github.com/Uzezi93/BIOL-607-Homework-Fall-2020-/blob/master/06_OKINEDO_UZEZI_2020.md) 
